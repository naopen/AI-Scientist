# Title: ESG Metric Extraction: Automated Identification and Normalization of Key Performance Indicators
# Experiment description: Develop a feature that incorporates machine learning models into the RAG system to automatically extract and standardize ESG-related quantitative indicators (such as CO2 emissions, employee diversity ratios, etc.). This will facilitate easier comparison of ESG performance across different companies and industries, enabling more objective analysis. Additionally, implement functionality to track extracted indicators over time, allowing for trend analysis.
## Run 0: Baseline
Results: {'shakespeare_char': {'final_train_loss_mean': 0.8081305821736654, 'best_val_loss_mean': 1.4677925109863281, 'total_train_time_mean': 205.99763210614523, 'avg_inference_tokens_per_second_mean': 420.12798409603414}, 'enwik8': {'final_train_loss_mean': 0.9315934181213379, 'best_val_loss_mean': 1.0042643547058105, 'total_train_time_mean': 1824.0911252498627, 'avg_inference_tokens_per_second_mean': 429.3039335196625}, 'text8': {'final_train_loss_mean': 0.9926142692565918, 'best_val_loss_mean': 0.9801740050315857, 'total_train_time_mean': 1791.0722711086273, 'avg_inference_tokens_per_second_mean': 428.0359768780263}}
Description: Baseline results.

## Run 3: Model Architecture Modification
Results: {'shakespeare_char': {'final_train_loss_mean': 0.81266188621521, 'best_val_loss_mean': 1.474806825319926, 'total_train_time_mean': 408.6268267631531, 'avg_inference_tokens_per_second_mean': 342.019111241075}, 'enwik8': {'final_train_loss_mean': 0.9977480173110962, 'best_val_loss_mean': 1.0281169414520264, 'total_train_time_mean': 2858.443796634674, 'avg_inference_tokens_per_second_mean': 338.6928307341758}, 'text8': {'final_train_loss_mean': 1.0432049036026, 'best_val_loss_mean': 1.0029417276382446, 'total_train_time_mean': 2778.0829932689667, 'avg_inference_tokens_per_second_mean': 344.9110870393106}}
Description: In Run 3, we modified the model architecture by increasing the number of layers and heads to 8 each. The goal was to assess whether a more complex model could improve performance on ESG data. The results showed a slight increase in training and validation loss across all datasets compared to the baseline, indicating that the increased complexity did not lead to better performance. However, the inference speed decreased, suggesting that the model's complexity impacted efficiency.

## Run 4: Dropout Rate Adjustment
Results: {'shakespeare_char': {'final_train_loss_mean': 1.0614346663157146, 'best_val_loss_mean': 1.4771403074264526, 'total_train_time_mean': 387.7747008005778, 'avg_inference_tokens_per_second_mean': 339.1847506778086}, 'enwik8': {'final_train_loss_mean': 1.0610493421554565, 'best_val_loss_mean': 1.0670863389968872, 'total_train_time_mean': 2821.2311594486237, 'avg_inference_tokens_per_second_mean': 339.00096718635035}, 'text8': {'final_train_loss_mean': 1.0922839641571045, 'best_val_loss_mean': 1.0372061729431152, 'total_train_time_mean': 2861.5989944934845, 'avg_inference_tokens_per_second_mean': 342.63789497987074}}
Description: In Run 4, we adjusted the dropout rate to 0.3 to explore its effect on model performance, aiming to reduce overfitting. The results showed an increase in both training and validation loss across all datasets compared to previous runs, indicating that the higher dropout rate may have negatively impacted the model's ability to learn effectively. The inference speed remained relatively stable.
