# Title: Dynamic Chunk Retrieval: Adaptive Document Segmentation for Improved ESG Context
# Experiment description: Implement a feature in the developed RAG system that dynamically segments ESG documents. Automatically adjust chunk sizes based on document content and questions to retrieve context containing more relevant ESG information. This is expected to enable more accurate and relevant ESG information extraction compared to traditional fixed-length chunk segmentation.
## Run 0: Baseline
Results: {'shakespeare_char': {'final_train_loss_mean': 0.8081305821736654, 'best_val_loss_mean': 1.4677925109863281, 'total_train_time_mean': 205.99763210614523, 'avg_inference_tokens_per_second_mean': 420.12798409603414}, 'enwik8': {'final_train_loss_mean': 0.9315934181213379, 'best_val_loss_mean': 1.0042643547058105, 'total_train_time_mean': 1824.0911252498627, 'avg_inference_tokens_per_second_mean': 429.3039335196625}, 'text8': {'final_train_loss_mean': 0.9926142692565918, 'best_val_loss_mean': 0.9801740050315857, 'total_train_time_mean': 1791.0722711086273, 'avg_inference_tokens_per_second_mean': 428.0359768780263}}
Description: Baseline results.

## Run 2: Dynamic Chunking Based on Content Density
Results: {'shakespeare_char': {'final_train_loss_mean': 0.8081642587979635, 'best_val_loss_mean': 1.4706518650054932, 'total_train_time_mean': 271.2708016236623, 'avg_inference_tokens_per_second_mean': 424.58824982938705}, 'enwik8': {'final_train_loss_mean': 0.9331086874008179, 'best_val_loss_mean': 1.0046145915985107, 'total_train_time_mean': 2031.4568166732788, 'avg_inference_tokens_per_second_mean': 407.52963557315945}, 'text8': {'final_train_loss_mean': 0.9980946183204651, 'best_val_loss_mean': 0.9791319370269775, 'total_train_time_mean': 2032.140261888504, 'avg_inference_tokens_per_second_mean': 417.86185332243565}}
Description: In this run, we implemented a dynamic chunking strategy based on content density. The chunk sizes were adjusted dynamically based on the density of ESG-related terms within the document. The results showed that this approach did not significantly improve the validation loss compared to the baseline. However, the training time increased, indicating that the dynamic chunking strategy may require more processing time without yielding better accuracy.

## Run 4: Advanced Dynamic Chunking with Machine Learning
Results: {'shakespeare_char': {'final_train_loss_mean': 2.239873011906942, 'best_val_loss_mean': 2.3087075551350913, 'total_train_time_mean': 193.09284575780234, 'avg_inference_tokens_per_second_mean': 426.6534973950692}, 'enwik8': {'final_train_loss_mean': 2.3696889877319336, 'best_val_loss_mean': 2.437260866165161, 'total_train_time_mean': 1966.6733367443085, 'avg_inference_tokens_per_second_mean': 425.2539341656864}, 'text8': {'final_train_loss_mean': 2.2091245651245117, 'best_val_loss_mean': 2.1760478019714355, 'total_train_time_mean': 1949.2548768520355, 'avg_inference_tokens_per_second_mean': 431.74111349742986}}
Description: In this run, we implemented an advanced dynamic chunking strategy that leverages machine learning to predict optimal chunk sizes. The `get_ml_predicted_chunk_size` function was used to dynamically adjust chunk sizes based on features extracted from the document content. Despite the innovative approach, the results showed an increase in both training and validation loss compared to previous runs. This suggests that the machine learning model may require further tuning or that the feature extraction process needs refinement. The training time was reduced compared to previous runs, indicating a more efficient processing pipeline.
